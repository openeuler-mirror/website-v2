---
title: Ceph社区动态（2022-6-1~2022-6-30）
date: 2022-7-8
tags:
    - Ceph
    - 动态
    - Pacific
    - OpenEuler
sig: sig-SDS
archives: 2022-7
author: rosinL
summary: Ceph社区动态
---
# Ceph社区动态（2022-6-1~2022-6-30）

## Cephalocon 2022 再次被迫延期
受限于当前COVID-19新冠疫情的影响，近期ceph的大量成员公司和大部分的社区成员均表示无法出行。

## Ceph v17.2.1 Quincy released
[v17.2.1](https://ceph.io/en/news/blog/2022/v17-2-1-quincy-released/)是Quincy版本的第一个bugfix版本，建议升级。
有如下主要更新：
- 使用全局配置参数bluestore_zero_block_detection，默认关闭"BlueStore zero block detection"特性。这个特性用于大范围的集成测试，但是与RBD和CephFS的一些特性不兼容。
- telemetry: 自动回传集群状态信息的模块，向'basic'模块中新增了一些度量项：Rook版本、Kubernetes版本、节点度量项等。
  - 当前回传数据的活跃集群达到1955个，总容量达到651PiB，详情请参看[dashboard](https://telemetry-public.ceph.com/)
- 在ceph-objectstore-tool中添加离线修剪pg_log副本功能
- 修正了在集群日志在滚动后未被填充的错误

## PG Autoscaler 模块简介
- pg_autoscaler模块最早是在14.2.x版本中引入，用于自动管理ceph集群中的PG。基于pool期望的使用情况，pg_autoscaler能够基于pool使用和用户的调优设置，给出调整建议并作出pg数量的调整。具体参见[说明文档](https://ceph.io/en/news/blog/2022/autoscaler_tuning/)
- pg_autoscaler的调优选项包括：--bulk 标志, target_size_ratio, noautoscale, bias, pg_num, pg_num_max, and pg_num_min.
  - --bulk 标志:解决了大集群创建之初pg数量较小导致的性能差的问题（没有--bulk标签时，集群默认创建最少的pg数量）.
  - target_size_ratio:设置某个pool占用的集群容量百分比
  - noautoscale: 用于全局的切换所有pool的autoscaler功能
  - Bias:用来帮助自动autoscaler准确地调整pg数量
  - pg_num:用于提前准确的设置pool的pg数量
  - pg_num_max:设置pool中pg最大数量
  - pg_num_min:设置pool中pg最小数量

## 近期社区合入pr
近期pr主要以bug修复为主，摘选了主要特性修改如下所示：
* BlueStore:
  - 在BlueStore中增加CoDel算法以缓解缓存膨胀问题[#46269](https://github.com/ceph/ceph/pull/46269)
  - 删除每个txc上的statfs更新,这降低了DB负载可以提升更好的性能[#46036](https://github.com/ceph/ceph/pull/46036)
- rgw
  - rgw原子写的benchmark,获取原子写的平均时间[#46822](https://github.com/ceph/ceph/pull/46822)
- librbd
  - 修复基于快照的rbd镜像，在快照数量达到上限时，可能导致数据损坏的问题[#46545](https://github.com/ceph/ceph/pull/46545)
- osd
  - 在ceph daemon中新增了导出pg log的接口，允许在集群运行过程中进行操作，解决了之前导出pg log需要停止集群的问题[#46571](https://github/com/ceph/ceph/pull/46571)
- crimson
  - 切换到GCC11[#46864](https://github.com/ceph/ceph/pull/46864)
  - 围绕回收进行调整和修复[#46437](https://github.com/ceph/ceph/pull/46437), 实现分代GC[#46436](https://github.com/ceph/ceph/pull/46436)，有三种GC策略可选：GREEDY、COST_BENEFIT、BENEFIT，rbd-fio用例稳定状态下整体带宽提升了11%
  - 活动扩展区检索的并行化[#46417](https://github.com/ceph/ceph/pull/46417),将poll的CPU开销从19%降低到2.5%，crimson-seastore的整体吞吐有了显著提升
- tracer
  - 默认情况下会编译tracer,并开展了一些优化以降低tracer对性能的影响[#44684](https://github.com/ceph/ceph/pull/44684)
- cephadm
  - 将默认的容器镜像源改为quay.io/ceph,此前是docker.io[#44894](https://github.com/ceph/ceph/pull/44894)
  - 在service spec中新增crush_device_class参数用来指定存储介质类型[#46480](https::/github.com/ceph/ceph/pull/46480)

## 近期Ceph Developer动态
Ceph社区各个模块会定期举行会议，讨论和对齐开发进展，会后有视频上传至[youtube](https://www.youtube.com/channel/UCno-Fry25FJ7B4RycCxOtfw/videos)，主要会议信息如下：
|会议名称|说明|频率|
|-------|----|----|
|Crimson SeaStore OSD Weekly Meeting |Crimson & Seastore开发周例会|周|
|Ceph Orchestration Meeting|Ceph管理模块（Mgr）开发|周|
|Ceph DocUBetter Meeting |文档优化|双周|
|Ceph Performance Meeting|Ceph性能优化|双周|
|Ceph Developer Monthly|Ceph开发者月度例会|月|
|Ceph Testing Meeting|版本验证及发布例会|月|
|Ceph Science User Group Meeting|Ceph科学计算领域会议|不定期|
|Ceph Leadership Team Meeting|Ceph领导团队例会|周|
|Ceph Tech talks|Ceph社区技术相关主题讨论|月|

### Performance Meeting
- tracer开发进展
  - 默认情况下会编译tracer, WITH_JAEGER=ON
  - 开展了一些优化措施以降低tracer编译或者开启导致的开销
  - 当前验证的性能结果是：tracer关闭时对于性能没有影响；tracer开启时，PUT性能降低~18%，GET性能降低~10%
- 关于snapmapper行为逻辑
  - 在正常关闭时,序列化SnapMapper并存储在BlueStore中的独立文件中,这将使关闭速度减慢少于一秒;
  - 启动时, 从文件重建SnapMapper,这将加快启动时间，因为读取文件比在RocksDB中迭代OMap对象快得多;
  - 在灾难场景，我们将从RocksDB中的ONode重建SNAPapper,我们在执行分配恢复时已经执行了ONodes迭代，因此构建SNAPMapper的额外开销将非常小，当前已经得到了大部分代码，可以简单地加入恢复循环

### ceph rbd_nvmeof Meeting
- 多RPC客户端的需求，不确定是否有收益，待后续跟踪
- 将grpc_server_max_workers硬编码为1个,暂无多线程需求[#25](https://github.com/ceph/ceph-nvmeof/pull/25)
- 增加REST API来访问nvmeof-gateway[#26](https://github.com/ceph/ceph-nvmeof/pull/26)
- spdk的rbd bdev存在核负载不均衡的问题[#10416](https://review.spdk.io/gerrit/c/spdk/spdk/+/10416)
- 讨论在SPDK中引入Ceph集群对象的core掩码，以允许将librbd的消息线程、io线程等固定到特定core
