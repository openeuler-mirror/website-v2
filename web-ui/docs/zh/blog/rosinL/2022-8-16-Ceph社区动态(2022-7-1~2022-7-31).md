---
title: Ceph社区动态	（2022-7-1~2022-7-30）
date: 2022-8-13
tags:
    -Ceph
    -动态
    -Pacific
    -OpenEuler
    sig:sig-SDS
    archives:2022-08
    author:fengchunsong
    summary:Ceph社区动态
---
# Ceph社区动态	（2022-7-1~2022-7-30）

## Quincy超大规模部署测试
使用200台服务器+100Gb网络部署quincy测试，主要评估集群管理、监控模块。发现了两个主要问题：Ceph守护进程上报性能计数导致mgr、CLI响应卡顿、横向扩展[`pg_autoscaler`](https://docs.ceph.com/en/quincy/rados/operations/placement-groups/#autoscaling-placement-groups)会影响pg间peering,都已经修复。使用cephadm管理大集群时，出错时错误信息过多、代理日志流量过大、集群管理交互困难，进行了优化。Prometheus管理大集群时无法一次收集所有监控节点，需要选择更好的SSD来存储数据。

## Ceph安全漏洞修复
修复如下两个安全漏洞，发布[v16.2.10](https://ceph.io/en/news/blog/2022/v16-2-10-pacific-released/)和[v17.2.2](https://ceph.io/en/news/blog/2022/v17-2-2-quincy-released/)版本，建议升级：

- 将其Ceph集群从Nautilus(或更早版本)升级到更高的主线版本，则很容易受到恶意用户的攻击。该漏洞允许用户访问CephFS文件系统层次结构的任意部分，而不是被正确限制在自己的子卷上。漏洞已修复。

- s3website请求不引用桶，从而出现空指针，导致RGW segfault,已修复。

## Ceph RocksDB深度调优
介绍了Ceph的默认RocksDB优化，并将其与其他几种配置进行了比较。我们研究了这些设置如何影响NVMe驱动器上的写入放大和性能，并试图展示不同配置选项之间的交互有多复杂。似乎通过正确的选项组合，可以在不显著增加写入放大的情况下实现更高的性能。在某种情况下，启用压缩可能会降低写入放大，并在某些测试中对性能产生中等影响。在这些测试中，通常性能最高的配置似乎是：
无压缩：

```
bluestore_rocksdb_options = compression=kNoCompression,max_write_buffer_number=128,min_write_buffer_number_to_merge=16,compaction_style=kCompactionStyleLevel,write_buffer_size=8388608,max_background_jobs=4,level0_file_num_compaction_trigger=8,max_bytes_for_level_base=1073741824,max_bytes_for_level_multiplier=8,compaction_readahead_size=2MB,max_total_wal_size=1073741824,writable_file_max_buffer_size=0
```
LZ4压缩（RGW负载可以减少写放大，但对桶列表性能有影响）:
```
bluestore_rocksdb_options = compression=kLZ4Compression,max_write_buffer_number=128,min_write_buffer_number_to_merge=16,compaction_style=kCompactionStyleLevel,write_buffer_size=8388608,max_background_jobs=4,level0_file_num_compaction_trigger=8,max_bytes_for_level_base=1073741824,max_bytes_for_level_multiplier=8,compaction_readahead_size=2MB,max_total_wal_size=1073741824,writable_file_max_buffer_size=0
```
这些非默认配置提升了性能，但还没有经过大量测试。在接下来的几个月里，我们将通过QA运行这些配置，并可能考虑更改Ceph下一个版本的默认值。

## 近期社区合入pr
近期pr主要以bug修复为主，摘选了主要特性修改如下所示：
* MemDB:
  - 为了简化buffer库，删除了MemDB代码，不再支持MemDB[#36282](https://github.com/ceph/ceph/pull/36282)
  - 删除每个txc上的statfs更新，这降低了DB负载可以提升更好的性能[#46036](https://github.com/ceph/ceph/pull/46036)
- compressor/zlib:
  
  - arm64 zlib使用isa-l优化性能提升4~7倍[#44762](https://github.com/ceph/ceph/pull/44762)
- libcephsqlite
  
  - 正规则表达式'-'在gcc8.5.0-14上编译会导致libcephsqlite崩溃，转义符修复[#47270](https://github.com/ceph/ceph/pull/47270)
- crimson
  
  - 解决停掉OSD时泄露ClientRequests[#47064](https://github.com/ceph/ceph/pull/47064)
  - 修复因重用移动对象而导致的sigsegv[#47237](https://github.com/ceph/ceph/pull/47237)
  - 修复Transaction::is_retired指针强制转换导致未定义行为[#47081](https://github.com/ceph/ceph/pull/47081)
- rgw
  
  - 优化抢锁，将5秒等待细分为多个10ms重试，减少持锁等待时间[#46248](https://github.com/ceph/ceph/pull/46248)
  - 增加s3website检查条件解决“未指定桶名时程序崩溃”问题[#46933](https://github.com/ceph/ceph/pull/46933)
- mgr大集群管理优化
  
  - 避免进度更新时dump所有pg统计信息[44208](https://github.com/ceph/ceph/pull/44208)
  
  - 限制对pg_num在pgp_num内更改[#44155](https://github.com/ceph/ceph/pull/44155)
  
  - 仅在模块需要通知时传递通知mgr[#44162](https://github.com/ceph/ceph/pull/44162)

## 近期Ceph Developer动态
Ceph社区各个模块会定期举行会议，讨论和对齐开发进展，会后有视频上传至[youtube](https://www.youtube.com/channel/UCno-Fry25FJ7B4RycCxOtfw/videos)，主要会议信息如下：
| 会议名称                            | 说明                         | 频率   |
| ----------------------------------- | ---------------------------- | ------ |
| Crimson SeaStore OSD Weekly Meeting | Crimson & Seastore开发周例会 | 周     |
| Ceph Orchestration Meeting          | Ceph管理模块（Mgr）开发      | 周     |
| Ceph DocUBetter Meeting             | 文档优化                     | 双周   |
| Ceph Performance Meeting            | Ceph性能优化                 | 双周   |
| Ceph Developer Monthly              | Ceph开发者月度例会           | 月     |
| Ceph Testing Meeting                | 版本验证及发布例会           | 月     |
| Ceph Science User Group Meeting     | Ceph科学计算领域会议         | 不定期 |
| Ceph Leadership Team Meeting        | Ceph领导团队例会             | 周     |
| Ceph Tech talks                     | Ceph社区技术相关主题讨论     | 月     |

### Performance Meeting
- v17.2.1比v17.2.0性能降低
  - 对比版本更改，找到bluestore的零块检测功能，在v17.2.0默认开启，在v17.2.1默认关闭。
- Ceph RocksDB深度调优
  - 主要讨论了rgw压缩，开启压缩写、删除性能较好，且写入流量更低。读性能使用lz4性能不会降低。如果限制rgw守护进程只使用2个核，查询桶列表时性能会降低，读、写、删除性能不变
  - 讨论RocksDB删除数据的墓碑清理、压缩策略，设置一个迭代窗口，在窗口内，超过阈值就压缩墓碑。
  - 讨论bluestore statfs统计信息优化，为即将实施的新WAL的做准备

### Crimson SeaStore OSD Weekly Meeting
- 修复zns支持相关问题
- GC事务不是由用户行为来源的，因此来自GC事务的范围读取操作不满足时间局部性原则。这些扩展区不应添加到LRU缓存中。

