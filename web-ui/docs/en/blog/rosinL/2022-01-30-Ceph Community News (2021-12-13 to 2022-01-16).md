---
title: Ceph Community News (2022-12-13 to 2022-01-16)
date: 2022-01-30
tags:
    - Ceph
    - News
    - Pacific
sig: ceph-sig
archives: 2022-1
author: rosinL
summary: Ceph Community News
---
# Ceph Community News (2022-12-13 to 2022-01-16)
## Qunicy Version Under Feature Freeze and to Be Released in March 2022
The Qunicy version has been under feature freeze since January 15, 2022, and is currently in the testing phase. In addition to regular tests, more scaling tests are performed this time. The test scale reaches 1000+ OSDs.
## Recently Merged PRs
- common: Optimized the PriorityCache. The cache is organized by time and priorities are much finer-grained. However, performance tests are showing little gain. [pr#43299](https://github.com/ceph/ceph/pull/43299)
- bluestore: Optimized the writing process. Zeros from the bufferlist are checked and skipped without writing. A new counter can be used to track this process. [pr#43337](https://github.com/ceph/ceph/pull/43337)
- bluestore: Reduced memory consumption for fsck during the startup. [pr#43667](https://github.com/ceph/ceph/pull/43667)
- bluestore: Optimized BlueFS locking at a finer granularity. [pr#43794](https://github.com/ceph/ceph/pull/43794)
- bluestore: Used the huge pageâ€“based read buffer, which is disabled by default. [pr#43691](https://github.com/ceph/ceph/pull/43691)
- mon: Added the --bulk flag when a pool is created so the pg autoscaler can adjust the initial number of PGs of the pool to the maximum value. By default, the pg autoscaler adjusts the initial number of PGs of the pool to the minimum value and dynamically changes the value. [pr#44241](https://github.com/ceph/ceph/pull/44241)
- mgr: Added the commands for enabling and disabling pg_autoscaler globally. [pr#43716](https://github.com/ceph/ceph/pull/43716)
- mgr: Implemented TTL cache to store cluster information and reduce the interaction with the cluster. [pr#44088](https://github.com/ceph/ceph/pull/44088)
- mgr: Optimized the balancer and refactored calc_pg_upmaps ([pr#44002](https://github.com/ceph/ceph/pull/44002)). The primary balancer is to be implemented in the Qunicy version to balance the capacity and load.
- cephdeduptool: Added chunk (offset and length) and object dedupe commands, facilitating the use of the deduplication function. [pr#43686](https://github.com/ceph/ceph/pull/43686)
- rgw/s3select: Added arrow/parquet ([pr#40802](https://github.com/ceph/ceph/pull/40802)). Due to the dependency problem, this function is disabled by default during compilation. [pr#44603](https://github.com/ceph/ceph/pull/44603)
- rgw: Added the user/bucket-level rate limit. [pr#42891](https://github.com/ceph/ceph/pull/42891)
- rgw: Reduced unnecessary bucket stats load. [pr#44538](https://github.com/ceph/ceph/pull/44538)
- rgw: Added the max_header_size option to adjust the parse buffer size of the beast. The default size is 16 KB, and the maximum size is 64 KB. A larger buffer size can reduce the number of socket reads ([pr#44029](https://github.com/ceph/ceph/pull/44029)). The size is increased to static 64 KB in [pr#29776](https://github.com/ceph/ceph/pull/29776), improving flexibility.
## Recent Ceph Developer News
Each module of the Ceph community holds regular meetings to discuss and align the development progress. Meeting videos are uploaded to [YouTube](https://www.youtube.com/channel/UCno-Fry25FJ7B4RycCxOtfw/videos). The major meetings are as follows:
|Meeting|Description|Frequency|
|-------|----|----|
|Crimson SeaStore OSD Weekly Meeting |Crimson & SeaStore development|Weekly|
|Ceph Orchestration Meeting|Ceph management module (mgr)|Weekly|
|Ceph DocUBetter Meeting |Document optimization|Biweekly|
|Ceph Performance Meeting|Ceph performance optimization|Biweekly|
|Ceph Developer Monthly|Ceph developers|Monthly|
|Ceph Testing Meeting|Version verification and release|Monthly|
|Ceph Science User Group Meeting|Ceph scientific computing|Irregularly|
|Ceph Leadership Team Meeting|Ceph leadership team|Weekly|

Recently, the focus is on performance. The following topics are discussed at the meetings:
- The performance test of the Quincy version needs to be compared with that of the Pacific version. The NVMe performance test is based on MAKO (AMD EPYC 7742) and 120 OSDs. The test covers single and multiple nodes (copy and EC). The test tools include fio(rbd cephfs), hsbench (rgw), tcmu-runner (iscsi/nbd), and omapbench(mds).
- The recovery testing may be added for the Qunicy version.
- According to the Pacific version test, perf cycles/op monitoring reduces the performance by up to 15%.
- The memory usage in the Ceph is not transparent because the memory of many modules cannot be tracked. Therefore, more memory optimization may be required.
