---
title: Ceph Community News (2022-01-17 to 2022-02-16)
date: 2022-02-23
tags:
    - Ceph
    - News
    - Pacific
sig: ceph-sig
archives: 2022-2
author: rosinL
summary: Ceph Community News
---
# Ceph Community News (2022-01-17 to 2022-02-16)
## Cephalocon 2022 Postponed
Due to the COVID-19 pandemic, Cephalocon 2022, originally scheduled for April 5-7 (April 6-8, Beijing Time), has been postponed. The new time is yet to be determined. The topics of the conference have been released and are listed below. For more details, see [Cephalocon 2022 schedule](https://ceph2022.sched.com/).
|Category|Topic|Speaker|Institution|
|----|-------|----|----|
|RGW, Performance|[Optimizing RGW Object Storage Mixed Media through Storage Classes and Lua Scripting](https://sched.co/w9FL)|Curt Bruns & Anthony D'Atri|Intel|
|RGW|[RGW: Sync What? Sync Info Provider: Early Peek](https://sched.co/w9Fm)|Yehuda Sadeh-Weinraub|Red Hat|
|RGW|[RGW â€“ An Ultimate S3 Frontend for MultipleBackends: An Implementation Story](https://sched.co/w9GJ)|Gregory Touretsky & Andriy Tkachuk|Seagate|
|RGW, S3select|[S3select: Computational Storage in S3](https://sched.co/w9GY)|Gal Salomon & Girjesh Rajoria|Red Hat|
|RGW|[Testing S3 Implementations: RGW & Beyond](https://sched.co/w9Gh)|Robin Hugh Johnson|DigitalOcean|
|RGW|[Introduction to Container Object Storage Interface aka COSI for ceph RGW](https://sched.co/w9Fs)|Jiffin Tony Thottan|Red Hat|
|RGW|[RGW Zipper](https://sched.co/w9GD)|Daniel Gryniewicz & Soumya Koduri|Red Hat|
|Cephadm|[Lightning Talk: Introduction to Cephadm](https://sched.co/w9EW)|Melissa Li|Red Hat|
|Dashboard|[Dashboard: Exploring Centralized Logging with Ceph Storage](https://sched.co/w9GP)|Gaurav Sitlani & Aashish Sharma|Red Hat|
|Dashboard|[Operating Ceph from the Ceph Dashboard: Past, Present and Future](https://sched.co/w9F0)|Ernesto Puerta|Red Hat|
|Ceph, QoS, mClock|[Ceph QoS Refinements for Background Operations using mClock](https://sched.co/w9Fv)|Sridhar Seshasayee|Red Hat|
|Ceph, PG|[pgremapper: CRUSHing Cluster OperationaComplexity](https://sched.co/w9EZ)|Joshua Baergen|DigitalOcean|
|Ceph, PG|[New Workload Balancer in Ceph](https://sched.co/w9Eo)|Josh Salomon & Laura Flores|Red Hat|
|Ceph, DPDK|[Lightning Talk: Ceph Messenger DPDkstack Development and Debugging](https://sched.co/w9FO)|Chunsong Feng|Huawei|
|Ceph, Windows|[Ceph on Windows](https://sched.co/w9Ei)|Alessandro Pilotti|Cloudbase Solutions|
|Seastore|[What's New with Crimson and Seastore?](https://sched.co/w9FI)|Samuel Just|Red Hat|
|Seastore, Crimson|[Lightning Talk: Introduction to Crimson from a Newbie](https://sched.co/w9FF)|Joseph Sawaya|Red Hat|
|Seastore|[Understanding SeaStore Through Profiling](https://sched.co/w9ET)|Yingxin Cheng & Tushar Gohad|Intel|
|Bluestore|[Accelerating PMEM Device Operations in BlueStore with Hardware Based Memory Offloading Technique](https://sched.co/w9F9)|Ziye Yang|Intel|
|Bluestore|[Revealing BlueStore Corruption Bugs in Containerized Ceph Clusters](https://sched.co/w9Fj)|Satoru Takeuchi|Cybozu|
|Dev|[Chasing Bad Checksums: A Journey through Ceph, TCMalloc, and the Linux kernel](https://sched.co/w9Fd)|Mauricio Faria de Oliveira & Dan Hill|Canonical|
|Dev|[Lightning Talk: Improving Ceph Build and Backport Automations Using Github Actions](https://sched.co/w9Gt)|Deepika Upadhyay|Red Hat|
|Dev|[Ceph Crash Telemetry Observability in Action](https://sched.co/w9Ec)|Yaarit Hatuka|Red Hat|
|Performance|[DisTRaC: Accelerating High-Performance Compute Processing for Temporary Data Storage](https://sched.co/w9Ef)|Gabryel Mason-Williams|Rosalind Franklin Institute|
|Performance|[Putting the Compute in your Storage](https://sched.co/w9Fg)|Federico Lucifredi & Brad Hubbard|Red Hat|
|Performance|[Modifying Ceph for Better HPC Performance](https://sched.co/w9Gb)|Darren Soothill|CROIT|
|Performance|[Over A Billion Requests Served Per Day: Ensuring Everyone is Happy with Our Ceph Clusters' Performance](https://sched.co/w9FR)|Jane Zhu & Matthew Leonard|Bloomberg LP|
|Performance|[Lessons Learned from Hardware Acceleration Initiatives for Ceph-specific Workloads](https://sched.co/w9G4)|Harry Richardson & Lionel Corbet|SoftIron|
|Performance|[The Effort to Exploit Modern SSDs on Ceph](https://sched.co/w9GG)|Myoungwon Oh|Samsung Electronics|
|Performance|[NVMe-over-Fabrics Support for Ceph](https://sched.co/w9GS)|Jonas Pfefferle, IBM & Scott Peterson|Intel|
|Security|[Introducing the New RBD Image Encryption Feature](https://sched.co/w9F3)|Or Ozeri & Danny Harnik|IBM|
|Security|[CephFS At-Rest Encryption with fscrypt](https://sched.co/w9Eu)|Jeffrey Layton|Red Hat|
|Security|[Secure Token Service in Ceph](https://sched.co/w9Ex)|Pritha Srivastava|Red Hat|
|Security|[Data Security and Storage Hardening in Rook and Ceph](https://sched.co/w9Fp)|Federico Lucifred & Michael Hackett|Red Hat|
|Ceph application|[Improved Business Continuity for an Existing Large Scale Ceph Infrastructure: A Story from Practical Experience](https://sched.co/w9G7)|Enrico Bocch & Arthur Outhenin-Chalandre|CERN|
|Ceph application|[How we Operate Ceph at Scale](https://sched.co/w9Fy)|Matt Vandermeulen|Digital Ocean|
|Ceph application|[BoF Session: Ceph in Scientific Computing and Large Clusters](https://sched.co/w9FC)|Kevin Hrpcek|Space Science & Engineering Center, University of Wisconsin - Madison|
|Ceph application|[Aquarium: An Easy to Use Ceph Appliance](https://sched.co/w9Ge)|Joao Eduardo Luis & Alexandra Settle|SUSE|
|Ceph application|[Stretch Clusters in Ceph: Algorithms, Use Cases, and Improvements](https://sched.co/w9Gn)|Gregory Farnum|Red Hat|
|Ceph application|[We Added 6 Petabytes of Ceph Storage and No Clients Noticed! Here's How We Did It.](https://sched.co/w9FX)|Joseph Mundackal & Matthew Leonard|Bloomberg LP|
|Ceph application|[Why We Built A "Message-Driven Telemetry System At Scale" Ceph Cluster](https://sched.co/w9FU)|Xiaolin Lin & Matthew Leonard|Bloomberg LP|
|Ceph application|[Lightning Talk: Ceph and 6G: Are We Ready for zettabytes?](https://sched.co/w9Gk)|Babar Khan|Technical University Darmstadt|
|Ceph application|[Bringing emails@ceph Into the Field](https://sched.co/w9G1)|Danny Al-Gaaf|Deutsche Telekom AG|
|Ceph application|[Lightning Talk: Ceph and QCOW2 a Match Made in Heaven: From Live Migration to Differential Snapshots](https://sched.co/w9F6)|Effi Ofer|IBM|
|Ceph application|[Lightning Talk: Installing Ceph on Kubernetes Using the Rook Operator and Helm](https://sched.co/w9GM)|Mike Petersen|Platform9|
|Benchmark|[Connecting The Dots: Benchmarking Ceph at Scale](https://sched.co/w9GA)|Shon Paz & Ido Pal|Red Hat|
|Benchmark|[Introducing Sibench: A New Open Source Benchmarking Optimized for Ceph](https://sched.co/w9GV)|Danny Abukalam|SoftIron|
## Recently Merged PRs
Recently,  PRs have mainly focused on bug fixing. The following describes notable changes:
- mgr: Disabled the pg recovery by default when OSD is in or out. You can manually enable it as required to reduce the impact on the service cluster. [pr#44588](https://github.com/ceph/ceph/pull/44588)
- osd: Added the dump_blocked_ops_count option to ceph daemon perf dump to quickly obtain the number of blocked ops, reducing the overhead caused by the dump_blocked_ops operation. [pr#44780](https://github.com/ceph/ceph/pull/44780)
- rgw: Added support for the conditional copy by the rgw s3 CopyObject interface. [pr#44678](https://github.com/ceph/ceph/pull/44678)
- rgw: Fixed the issue that a large amount of memory is used during the radosgw-admin bucket chown process. [pr#44357](https://github.com/ceph/ceph/pull/44357)
- rbd: Introduced the rxbounce option to krbd to solve the problem of CRC errors and performance deterioration when images are used as block devices of the Windows system. [pr#44842](https://github.com/ceph/ceph/pull/44842)
## Recent Ceph Developer News
Each module of the Ceph community holds regular meetings to discuss and align the development progress. Meeting videos are uploaded to [YouTube](https://www.youtube.com/channel/UCno-Fry25FJ7B4RycCxOtfw/videos). The major meetings are as follows:
|Meeting|Description|Frequency|
|-------|----|----|
|Crimson SeaStore OSD Weekly Meeting |Crimson & SeaStore development|Weekly|
|Ceph Orchestration Meeting|Ceph management module (mgr) development|Weekly|
|Ceph DocUBetter Meeting |Document optimization|Biweekly|
|Ceph Performance Meeting|Ceph performance optimization|Biweekly|
|Ceph Developer Monthly|Ceph developers|Monthly|
|Ceph Testing Meeting|Version verification and release|Monthly|
|Ceph Science User Group Meeting|Ceph scientific computing|Irregularly|
|Ceph Leadership Team Meeting|Ceph leadership team|Weekly|

Recently, the community focuses on the freeze test and verification of the Quincy version. The following topics are discussed at the meetings:
- In the Quincy version test, the read performance meets the expectation, but the write performance deteriorates in some scenarios. It can be determined that 4k min_alloc_size and bluestore allocator can improve the performance.
- As the omap scale increases, the omap_iterator efficiency causes a large number of slow_ops or even no response. The [issue](https://tracker.ceph.com/issues/53926) records the test results of the two compaction modes. If the compaction is manually triggered, the latency cannot be restored to the previous level. To resolve this issue, Rocksdb provides [periodic and TTL compaction](https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide#periodic-and-ttl-compaction). After this function is enabled, the latency can be restored to the previous level.
- A large number of PRs are merged for the Ceph Benchmark Tool (CBT) with a focus on the control of memory resources during the large-scale OSD test and the multi-client concurrent test cases.
